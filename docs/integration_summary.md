# Uno Framework Integration Summary

## Introduction

This document summarizes the integration architecture between the key components of the Uno framework: UnoObj, UnoModel, UnoDB, DBManager, and UnoEndpoint. This integration forms the foundation of the library, enabling a clean separation of concerns while maintaining a cohesive data flow.

## Key Components

### UnoModel

- SQLAlchemy-based ORM models for database tables
- Provides type annotations for database columns
- Handles database schema representation
- Leverages PostgreSQL-specific types for optimized storage

### UnoObj

- Pydantic-based business logic models
- Validates data through Pydantic's validation system
- Manages object lifecycle (create, read, update, delete)
- Provides a clean API for business logic operations

### UnoDB

- Factory-generated database access layer
- Handles database operations (queries, inserts, updates, deletes)
- Manages database transactions and error handling
- Provides specialized functionality like merge operations

### DBManager

- Manages database structure through DDL operations
- Handles database objects like schemas, functions, and triggers
- Validates and executes SQL statements with built-in security checks
- Integrates with SQL Emitters for standardized SQL generation

### UnoEndpoint

- FastAPI-based API endpoints
- Exposes business objects through RESTful interfaces
- Handles request validation and response serialization
- Integrates with the schema system for data transformation

## Integration Architecture

The integration between these components follows a well-defined pattern that ensures data integrity and proper transformation at each step.

### Data Flow

1. **UnoObj Creation (Starting Point)**
   - Create an instance of UnoObj with data
   - Data is validated through Pydantic validation
   - Business logic can be applied at this stage

2. **UnoObj -> Schema -> UnoModel Conversion**
   - UnoObj uses the schema system to convert to a model instance
   - A Pydantic schema is generated by the UnoSchemaManager
   - UnoObj data is first validated through the schema
   - Conversion happens via the `to_model(schema_name)` method
   - Different schemas can be used for different operations (view, edit, etc.)

3. **UnoModel -> UnoDB Database Operations**
   - The UnoModel is passed to the appropriate UnoDB operation
   - Operations include create, update, delete, get, filter, and merge
   - Database-specific functionality is encapsulated here

4. **Database Results -> UnoModel -> UnoObj Conversion**
   - Results from the database are converted back to UnoObj instances
   - This happens automatically in methods like `get()` and `filter()`
   - Business logic objects are populated with database data

5. **UnoObj -> UnoEndpoint Integration**
   - UnoEndpoint exposes operations like create, read, update, delete
   - Endpoints use UnoObj methods to interact with the business layer
   - Endpoints automatically use schemas for request/response transformation
   - The UnoEndpointFactory creates standardized endpoints for UnoObj classes

### Complete Roundtrip Flow

```
HTTP Request → UnoEndpoint → UnoObj → Schema → UnoModel → UnoDB → Database```
```

                                                            ↓
```
```
HTTP Response ← UnoEndpoint ← UnoObj ← Schema ← UnoModel ← UnoDB ←
```

### Key Integration Methods

- **UnoObj.to_model(schema_name)**: Converts a UnoObj to a UnoModel using a schema
- **UnoObj.from_model(model)**: Creates a UnoObj from a UnoModel or schema
- **UnoObj.save()**: Creates or updates an object in the database
- **UnoObj.delete()**: Removes an object from the database
- **UnoObj.merge()**: Determines whether to insert or update based on existence
- **UnoObj.get()**: Retrieves an object from the database
- **UnoObj.filter()**: Returns multiple objects matching filter criteria
- **UnoEndpointFactory.create_endpoints()**: Creates standardized endpoints for a UnoObj

## Benefits of This Architecture

1. **Separation of Concerns**
   - Business logic remains in UnoObj
   - Database schema configuration stays in UnoModel
   - Database operations are isolated in UnoDB
   - API concerns are managed in UnoEndpoint

2. **Data Validation**
   - Pydantic validation ensures data integrity
   - Schemas provide controlled data transformations
   - Type annotations provide IDE support and runtime validation
   - API input validation through FastAPI's integration with Pydantic

3. **Flexible Schema Management**
   - Different schemas for different operations
   - Control over field inclusion/exclusion
   - Customized validation rules per schema
   - Automatic documentation in API endpoints

4. **Clean API**
   - Intuitive methods for common operations
   - Consistent patterns across the framework
   - Clear responsibilities for each component
   - Standardized RESTful endpoint patterns

## Testing Approach

When testing this integration, focus on:

1. **Data Transformation**: Test the conversion between UnoObj and UnoModel
2. **Database Operations**: Test the interaction with UnoDB for data access
3. **DDL Operations**: Test the DBManager for database structure management
4. **Roundtrip Flow**: Test the complete flow from object to database and back
5. **API Endpoints**: Test the REST API endpoints using FastAPI TestClient
6. **Full Stack Integration**: Test the complete flow from API request to database and back

### Component Integration Testing

For testing the integration between individual components:

```python
class TestUserIntegration(IsolatedAsyncioTestCase):```

async def test_create_superuser_obj_model_conversion(self):```

# Create a superuser UnoObj instance
superuser = User(
    email="test@example.com",
    handle="test",
    full_name="Test User",
    is_superuser=True
)
``````

```
```

# Ensure schemas are created
superuser._ensure_schemas_created()
``````

```
```

# Convert to UnoModel using the schema
db_model = superuser.to_model(schema_name="edit_schema")
``````

```
```

# Verify UnoModel instance
assert isinstance(db_model, UserModel)
assert db_model.email == "test@example.com"
assert db_model.is_superuser is True
```
```
```

### Endpoint Integration Testing

For testing endpoint integration with mocks:

```python
class TestEndpointIntegration(IsolatedAsyncioTestCase):```

def setUp(self):```

# Create FastAPI app and test client
self.app = FastAPI()
self.client = TestClient(self.app)
``````

```
```

# Create endpoints for User model
factory = UnoEndpointFactory()
factory.create_endpoints(
    app=self.app,
    model_obj=User,
    endpoints=["View", "List"]
)
```
``````

```
```

@patch("uno.authorization.objs.User.get")
async def test_view_endpoint(self, mock_get):```

# Mock the User.get method to return test data
user_data = {"id": "123", "email": "test@example.com"}
mock_get.return_value = user_data
```
    ```

# Make a request to the endpoint
response = self.client.get("/api/v1/user/123")
```
    ```

# Verify the response
assert response.status_code == 200
assert response.json() == user_data
```
```
```

### Concept-Based Integration Testing

When complex component interactions make direct testing challenging, you can use concept-based tests that focus on verifying the integration patterns without being tied to specific implementation details:

```python
class TestEndpointIntegrationConcepts(IsolatedAsyncioTestCase):```

async def test_view_endpoint_concept(self):```

"""
Test the concept of the View endpoint.
```
    ```

This test demonstrates how data flows through a View endpoint
from database to API.
"""
# 1. Client requests an object by ID
object_id = "test123"
```
    ```

# 2. UnoObj.get() method is called with the ID
db_result = await MockUser.get(object_id)
```
    ```

# 3. Result is converted to a schema for response
response_schema = UserViewSchema(**db_result)
```
    ```

# 4. Schema is returned to the client as JSON
response_data = response_schema.model_dump()
```
    ```

# Verify response
assert response_data["id"] == object_id
assert response_data["email"] == "test@example.com"
```
```
```

This approach allows you to verify integration patterns without getting entangled in complex mocking or actual database/network operations. It's especially valuable when:

1. The component interactions are complex or deeply nested
2. Circular dependencies make mocking difficult
3. You want to focus on the conceptual flow rather than implementation details

## Best Practices

1. **Define Clear Model Relationships**: Explicitly set the model reference in UnoObj subclasses
2. **Create Appropriate Schemas**: Define schemas that match your use cases
3. **Test the Full Roundtrip**: Ensure objects survive the full roundtrip
4. **Add Business Logic Methods**: Implement domain-specific operations as methods
5. **Handle Database Errors**: Properly catch and handle database exceptions
6. **Consider Authorization**: Design with security in mind
7. **Use the EndpointFactory**: Create standardized endpoints using the factory
8. **Document Endpoints**: Add proper documentation to your API endpoints
9. **Test API Endpoints**: Create integration tests for your API endpoints
10. **Validate API Inputs**: Ensure all API inputs are properly validated

## Conclusion

The integration between UnoObj, UnoModel, UnoDB, DBManager, and UnoEndpoint provides a solid foundation for the Uno framework. By maintaining a clear separation of concerns while ensuring smooth data flow between components, the architecture promotes maintainable, testable, and flexible code.

This architecture allows for business logic to be developed independently of database concerns and API implementation, while still ensuring that data is properly validated, stored, and exposed through RESTful interfaces. The schema-based approach to data transformation provides additional flexibility for different use cases and API requirements.

The complete integration pattern allows developers to:

1. Define business objects with UnoObj
2. Map them to database models with UnoModel
3. Manage database structure with DBManager
4. Perform data operations with UnoDB
5. Expose functionality through API endpoints with UnoEndpoint

This comprehensive integration provides a complete solution for building data-driven applications with a clean separation of concerns.